import streamlit as st
import requests
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import pandas as pd
import re

API_KEY = 'AIzaSyBGiCgfY5Vjyh7j5xoYr__fwb1E1vSBxWA'

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")
    model = AutoModelForSequenceClassification.from_pretrained("beomi/KcELECTRA-base")
    return tokenizer, model

tokenizer, model = load_model()

def classify_comment(comment):
    inputs = tokenizer(comment, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        label = torch.argmax(probs).item()
    return label, probs.tolist()[0]

def extract_video_id(url):
    match = re.search(r"(?:v=|youtu\.be/)([a-zA-Z0-9_-]{11})", url)
    return match.group(1) if match else None

def get_comments(video_id):
    comments = []
    next_page_token = None
    while True:
        base_url = (
            f"https://www.googleapis.com/youtube/v3/commentThreads"
            f"?part=snippet&videoId={video_id}&key={API_KEY}&maxResults=100"
        )
        if next_page_token:
            base_url += f"&pageToken={next_page_token}"

        res = requests.get(base_url)
        data = res.json()

        for item in data.get("items", []):
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)

        next_page_token = data.get("nextPageToken")
        if not next_page_token:
            break
    return comments

# Streamlit UI
st.title("ğŸ§  YouTube ëŒ“ê¸€ ê°ì„± ë¶„ì„ê¸°")
video_url = st.text_input("YouTube ì˜ìƒ URL ì…ë ¥")

if st.button("ë¶„ì„ ì‹œì‘"):
    with st.spinner("ëŒ“ê¸€ ë¶„ì„ ì¤‘..."):
        video_id = extract_video_id(video_url)
        if not video_id:
            st.error("URLì´ ì´ìƒí•œë°?")
        else:
            try:
                comments = get_comments(video_id)
                st.write(f"ì´ ëŒ“ê¸€ ìˆ˜: {len(comments)}ê°œ")

                results = []
                label_map = {0: "ë¶€ì •", 1: "ê¸ì •"}
                status_text = st.empty()
                progress_bar = st.progress(0)

                for idx, c in enumerate(comments):
                    try:
                        label, probs = classify_comment(c)
                        results.append({
                            "ëŒ“ê¸€": c,
                            "ê°ì„±": label_map[label],
                            "ë¶€ì • í™•ë¥ ": round(probs[0], 3),
                            "ê¸ì • í™•ë¥ ": round(probs[1], 3)
                        })
                    except Exception:
                        continue

                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    percent_complete = int((idx + 1) / len(comments) * 100)
                    progress_bar.progress(percent_complete)
                    status_text.text(f"{idx + 1} / {len(comments)}ê°œ ë¶„ì„ ì™„ë£Œ")

                df = pd.DataFrame(results)
                st.dataframe(df)

            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")
