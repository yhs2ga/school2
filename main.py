import streamlit as st
import requests
import re
from Korpora import Korpora
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# 1. í•™ìŠµëœ ëª¨ë¸ ìºì‹œë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_resource
def train_model():
    corpus = Korpora.load("korean_hate_speech")

    texts = []
    labels = []

    for d in corpus.train:
        try:
            label = d.__dict__.get("label", None)
            text = d.__dict__.get("text", None)
            if label and text:
                texts.append(text)
                labels.append(1 if label in ["hate", "offensive"] else 0)
        except:
            continue

    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X = vectorizer.fit_transform(texts)
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X, labels)

    return vectorizer, clf

vectorizer, model = train_model()

# 2. ìœ íŠœë¸Œ API í‚¤ ì„¤ì •
API_KEY = 'AIzaSyCEPm16vLDOuCxBH7eXB8_c8Kk78kfKfJQ'

# 3. ìœ íŠœë¸Œ URLì—ì„œ video ID ì¶”ì¶œ
def extract_video_id(url):
    match = re.search(r"(?:v=|youtu\\.be/|embed/|shorts/)([0-9A-Za-z_-]{11})(?:[&?\\s]|$)", url)
    return match.group(1) if match else None

# 4. ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_comments(video_id):
    comments = []
    next_page_token = None

    while True:
        url = f"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={API_KEY}&maxResults=100"
        if next_page_token:
            url += f"&pageToken={next_page_token}"

        res = requests.get(url)
        if res.status_code != 200:
            break

        data = res.json()
        for item in data.get("items", []):
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)

        next_page_token = data.get("nextPageToken")
        if not next_page_token:
            break

    return comments

# 5. ëŒ“ê¸€ ë¶„ë¥˜ í•¨ìˆ˜
def classify_comments(comments):
    X = vectorizer.transform(comments)
    preds = model.predict(X)
    return [(c, int(p)) for c, p in zip(comments, preds)]

# 6. Streamlit ì•± UI
st.title("ğŸ§¹ ìœ íŠœë¸Œ ì•…í”Œ í•„í„°ê¸° (Korpora ëª¨ë¸ ì‹¤ì‹œê°„ í•™ìŠµ)")
url = st.text_input("YouTube ì˜ìƒ URL ì…ë ¥")

if st.button("ì•…í”Œ ë¶„ì„ ì‹œì‘"):
    with st.spinner("ëŒ“ê¸€ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ + ëª¨ë¸ í•™ìŠµ ì¤‘..."):
        video_id = extract_video_id(url)
        if not video_id:
            st.error("ìœ íš¨í•œ ìœ íŠœë¸Œ URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        else:
            try:
                comments = get_comments(video_id)
                st.write(f"ì´ ëŒ“ê¸€ ìˆ˜: {len(comments)}ê°œ")
                results = classify_comments(comments)
                hate_comments = [c for c, label in results if label == 1]
                st.success(f"ì•…í”Œ ê°ì§€ë¨: {len(hate_comments)}ê°œ")
                for hc in hate_comments:
                    st.write(f"- {hc}")
            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")
