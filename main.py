import streamlit as st
import requests
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import pandas as pd
import re

# ğŸ”‘ ìœ íŠœë¸Œ API í‚¤ ì…ë ¥
API_KEY = 'ë„ˆì˜_API_KEY'

# ğŸ§  ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ (ìºì‹±)
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")
    model = AutoModelForSequenceClassification.from_pretrained("beomi/KcELECTRA-base")
    return tokenizer, model

tokenizer, model = load_model()

# ğŸ¯ ìœ íŠœë¸Œ ë§í¬ì—ì„œ ì˜ìƒ ID ì¶”ì¶œ
def extract_video_id(url):
    match = re.search(r"(?:v=|youtu\.be/)([a-zA-Z0-9_-]{11})", url)
    return match.group(1) if match else None

# ğŸ—£ ëŒ“ê¸€ ë¶ˆëŸ¬ì˜¤ê¸°
def get_comments(video_id):
    comments = []
    next_page_token = ''
    while True:
        url = (
            f"https://www.googleapis.com/youtube/v3/commentThreads"
            f"?part=snippet&videoId={video_id}&key={API_KEY}&maxResults=100&pageToken={next_page_token}"
        )
        res = requests.get(url)
        data = res.json()
        for item in data.get("items", []):
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)
        next_page_token = data.get("nextPageToken")
        if not next_page_token:
            break
    return comments

# ğŸ§ª ê°ì„± ë¶„ë¥˜ (0: ë¶€ì •, 1: ê¸ì •)
def classify_comment(comment):
    inputs = tokenizer(comment, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        label = torch.argmax(probs).item()
    return label, probs.tolist()[0]

# ğŸš€ Streamlit UI
st.title("ğŸ§  YouTube ëŒ“ê¸€ ê°ì„± ë¶„ì„ê¸°")

video_url = st.text_input("YouTube ì˜ìƒ URL ì…ë ¥")

if st.button("ë¶„ì„ ì‹œì‘"):
    with st.spinner("ëŒ“ê¸€ ë¶„ì„ ì¤‘..."):
        video_id = extract_video_id(video_url)
        if not video_id:
            st.error("URL ì´ìƒí•¨.")
        else:
            try:
                comments = get_comments(video_id)
                st.write(f"ì´ ëŒ“ê¸€ ìˆ˜: {len(comments)}ê°œ")

                results = []
                label_map = {0: "ë¶€ì •", 1: "ê¸ì •"}

                for c in comments:
                    label, probs = classify_comment(c)
                    results.append({
                        "ëŒ“ê¸€": c,
                        "ê°ì„±": label_map[label],
                        "ë¶€ì • í™•ë¥ ": round(probs[0], 3),
                        "ê¸ì • í™•ë¥ ": round(probs[1], 3)
                    })

                df = pd.DataFrame(results)
                st.dataframe(df)

            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")
