import streamlit as st
import requests
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import pandas as pd
import re

API_KEY = 'ë„ˆì˜_API_KEY'

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")
    model = AutoModelForSequenceClassification.from_pretrained("beomi/KcELECTRA-base")
    return tokenizer, model

tokenizer, model = load_model()

def classify_comment(comment):
    inputs = tokenizer(comment, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        label = torch.argmax(probs).item()
    return label, probs.tolist()[0]  # (ë¼ë²¨, í™•ë¥  ë¦¬ìŠ¤íŠ¸)

def extract_video_id(url):
    match = re.search(r"(?:v=|youtu\.be/)([a-zA-Z0-9_-]{11})", url)
    return match.group(1) if match else None

def get_comments(video_id):
    comments = []
    next_page_token = ''
    while True:
        url = (
            f"https://www.googleapis.com/youtube/v3/commentThreads"
            f"?part=snippet&videoId={video_id}&key={API_KEY}&maxResults=100&pageToken={next_page_token}"
        )
        res = requests.get(url)
        data = res.json()

        for item in data.get("items", []):
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)

        next_page_token = data.get("nextPageToken")
        if not next_page_token:
            break

    return comments

# UI
st.title("ğŸ§  YouTube ëŒ“ê¸€ ê°ì„± ë¶„ì„ê¸°")
video_url = st.text_input("YouTube ì˜ìƒ URLì„ ì…ë ¥í•˜ì„¸ìš”")

if st.button("ë¶„ì„ ì‹œì‘"):
    with st.spinner("ëŒ“ê¸€ ë¶„ì„ ì¤‘..."):
        video_id = extract_video_id(video_url)
        if not video_id:
            st.error("URL ì´ìƒí•¨.")
        else:
            try:
                comments = get_comments(video_id)
                st.write(f"ëŒ“ê¸€ {len(comments)}ê°œ ë¶„ì„ ì¤‘...")

                results = []
                label_map = {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"}

                for c in comments:
                    label, probs = classify_comment(c)
                    results.append({
                        "ëŒ“ê¸€": c,
                        "ê°ì„±": label_map[label],
                        "ë¶€ì • í™•ë¥ ": round(probs[0], 3),
                        "ì¤‘ë¦½ í™•ë¥ ": round(probs[1], 3),
                        "ê¸ì • í™•ë¥ ": round(probs[2], 3),
                    })

                df = pd.DataFrame(results)
                st.dataframe(df)

                csv = df.to_csv(index=False).encode("utf-8-sig")
                st.download_button("CSVë¡œ ì €ì¥í•˜ê¸°", csv, "comment_analysis.csv", "text/csv")

            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")
